{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ead512",
   "metadata": {},
   "source": [
    "# 0. Motivation\n",
    "\n",
    "In the previous section, we compared the run time between an iterative and recursive solution for computing Fibonacci numbers. We also compared the run time between the bubble sort and quick sort algorithms. These comparisons shed light on a fundamental question, \"How long will a program take to run?\" The answer to this question depends on various factors, including computer memory, processing speed, and the size of the input, among others. Additionally, it's essential to understand how run time scales as the size of the input increases. For instance, does computing the *20-th* Fibonacci number require twice as much time as computing the *10-th* Fibonacci number, or is it more/less than that?\n",
    "\n",
    "The effort required to run a program to completion is known as **complexity**. By the end of this section, you will have the skills to:\n",
    "\n",
    "* Estimate the complexity of simple programs\n",
    "* Identify algorithms with poor complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240444df",
   "metadata": {},
   "source": [
    "# 1. Complexity\n",
    "\n",
    "Complexity in the context of algorithms refers to the relationship between the size of the input and the effort required to execute the algorithm to completion. In general, the complexity of any algorithm is determined by both its time complexity and space complexity. Time complexity refers to the time needed by an algorithm to complete its execution relative to the size of the input. Meanwhile, space complexity refers to the total amount of memory used by the algorithm to complete its execution. Since execution time can vary due to differences in hardware configurations across machines, we aim to measure complexity in a machine-independent manner.\n",
    "\n",
    "Complexity of a problem can be measured in several ways. One common approach is to count the number of **basic operations** such as: additions, subtractions, multiplications, divisions, assignments, and function calls. While these operations may have varying execution times, the number of basic operations needed to complete an algorithm is sufficiently related to the run time, and is much easier to count. Let's look at an example.\n",
    "\n",
    "## 1.1. Linear Search\n",
    "\n",
    "Imagine we are searching to check whether some number `x` exists in a list `L`. For example, we could have `x = 5` and we want to check if `x` is in list `L = [12, 50, 24, 4, 55, 3, 15, 21, 79, 16]`. If we didn't have the Python `in` operator (`x in L`), we could implement the following function:\n",
    "\n",
    "```Python\n",
    "\n",
    "def lin_search(x, L):\n",
    "    \"\"\"return True if x is in list L\"\"\"\n",
    "    for item in L:\n",
    "        if item == x: # only returns True if x is found in L\n",
    "            return True\n",
    "    return False # only gets here if x is not found in L after checking all values in L\n",
    "\n",
    "```\n",
    "\n",
    "This function exemplifies Linear Search, which is a sequential search algorithm that starts at one end and goes through each element of a list until it finds the desired element, or until it reaches the end of the list. Next, let's analyze the complexity of the `lin_search()` function. \n",
    "\n",
    "## 1.2. Linear Search Complexity\n",
    "\n",
    "How many steps will it take to determine if `x` exists in a list with $n$ items? In the worst-case scenario, a linear search function checks each item in the list. So, if `len(L) = 10`, then there are, at worst, 10 comparisons (`if item == x` is checked 10 times). While there are a few more steps in that function (e.g., returning True or False), but the for loop is the primary part.\n",
    "\n",
    "If we double the size of the list `L`, this will double the number of steps required, again considering the worst scenario. If `x` happens to be the first item in `L`, then only $1$ step is needed, but this is the best-case scenario. A possibly more realistic measure would be the average case, requiring $n/2$ comparisons. Nevertheless, this figure is directly dependent on the size of the list, $n$. So, if we double the size of the list, linear search will require, on average, or in the worst case, twice as many steps.\n",
    "\n",
    "This algorithm is therefore known as a **linear algorithm**, because it exhibits a linear dependence between the size of the input and the number of steps required to complete the execution. If we were to plot the number of steps needed (at worst) versus the size of the list, it would be a straight (linear) line.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src=\"https://content.codecademy.com/practice/art-for-practice/new-pngs/linear-search-graph.png\" style=\"width:40%\">\n",
    "    <figcaption style=\"text-align:center\"><strong>Complexity of linear search algorithm:</strong> <a href=\"https://www.codecademy.com/learn/linear-data-structures-java/modules/searching-arrays-java/cheatsheet\">https://www.codecademy.com/</a></figcaption>   \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2bad22",
   "metadata": {},
   "source": [
    "# 2. Big-O Notation\n",
    "\n",
    "Big-O notation is a widely used method for describing the growth in the number of basic operations (i.e., steps) as a function of the input size, particularly as the input size becomes very large. For example, saying that the complexity is $O(n)$ or \"order n\" means that the time required to execute the program increases *in proportion* to $n$.\n",
    "\n",
    "\n",
    "## 2.1. Linear Complexity $O(n)$\n",
    "\n",
    "As we've observed with the `lin_search()` algorithm, it requires $n$ steps for a list of size $n$ in the worst case. This precisely aligns with the concept of Big-O notation, which represents the upper bound, or worst-case complexity, of the number of steps as a function of the input size \n",
    "\n",
    "In the case of the `lin_search()`, its Big-O (worst-case) run time is denoted as $O(n)$. This means that as the input size increases, the number of operations increases linearly. Any algorithm with complexity $O(n)$ exhibits **linear complexity**.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>TRY IT!</b> Determine the complexity of the iterative Fibonacci function in Big-O notation.</div>\n",
    "\n",
    "``` Python\n",
    "def fibonacci_iter(n):\n",
    "    fib = np.ones(n)\n",
    "    for i in range(2, n):\n",
    "        fib[i] = fib[i - 1] + fib[i - 2]\n",
    "    return fib[-1]\n",
    "```\n",
    "\n",
    "Since the only lines of code that take more time as $n$ grows are those in the for-loop, we can restrict our attention to the for-loop and the code block within it. The code within the for-loop is only repeating an operation based on the size of $n$, and thus, does not grow further. In each iteration, we are performing one assignment and one addition, and thus we have $2$ basic operations\n",
    "\n",
    "There are $n-2$ iterations (from $2$ to $n-1$ inclusive). Thus the total number of basic operations that occurs is $2(n-2)$. Thus, one can write that the complexity is $O(2n - 4)$.\n",
    "\n",
    "When representing complexity using Big-O notation, we focus on the most dominant part. Therefore, coefficients are not required to characterize growth and are dropped. This gives a complexity of $O(n)$ for `fibonacci_iter()`, which makes the algorithm not efficient for large inputs.\n",
    "\n",
    "## 2.2. Polynomial Complexity $O\\left(n^c\\right)$\n",
    "\n",
    "As mentioned earlier, when expressing complexity using Big-O notation, we focus on the most dominant term within the expression. Therefore, only the highest power term is included in Big-O notation. For instance, if we counted $4n^2 + n + 1$ operations for an algorithm, in Big-O notation, we would say that the complexity is $O\\left(n^2\\right)$ (pronounced \"O of n-squared\", quadratic complexity). This means that the number of steps quadruples each time the size of data doubles. \n",
    "\n",
    "In general, any algorithm with complexity $O\\left(n^c\\right)$ where $c$ is some constant has **polynomial complexity**. This can take various forms, such as $O\\left(n^2\\right)$, $O\\left(n^3\\right)$, and so on. \n",
    "\n",
    "To illustrate, let's consider the following function:\n",
    "\n",
    "$$f(n) = \\sum_{i=1}^{n}\\sum_{j=1}^{n}i\\cdot j$$\n",
    "\n",
    "For example, if $n=2$:\n",
    "\n",
    "$$f(n) = \\sum_{i=1}^{2}\\sum_{j=1}^{2}i\\cdot j= 1 \\times 1 + 1 \\times 2 + 2 \\times 1 +  2 \\times 2  = 9$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>TRY IT!</b> Write a function to evaluate $f(n)$. Determine the complexity of this function in Big-O notation.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036331ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function\n",
    "def f(n):\n",
    "    out = 0\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            out = out + i * j\n",
    "            \n",
    "    return out\n",
    "    \n",
    "# call the function    \n",
    "f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d556f2a",
   "metadata": {},
   "source": [
    "In the above example, the only parts of code that take more time as $n$ grows are those in the nested loop. So, we can focus on the code block within the nested loop. The inner loop is repeating an operation $n$ times (from $j=1, 2, ..., n$) for every iteration in the outer loop, which also repeats $n$ times (from $i=1, 2, ..., n$). Therefore, the total number of operations is $n \\times n = n^2$. This gives a complexity of $O\\left(n^2\\right)$ for `f()`, which is considered very expensive.\n",
    "\n",
    "## 2.3. Exponential Complexity $O\\left(c^n\\right)$\n",
    "\n",
    "Assessing the exact complexity of a function can be difficult. In these cases, it might be sufficient to give an upper bound or even an approximation of the complexity.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>TRY IT!</b> Give an upper bound on the complexity of the recursive Fibonacci function in Big-O notation.</div>\n",
    "\n",
    "``` Python\n",
    "\n",
    "def fibonacci_rec(n):\n",
    "    if n == 1: # first base case\n",
    "        return 1\n",
    "    elif n == 2: # second base case\n",
    "        return 1\n",
    "    else: # Recursive step\n",
    "        return fibonacci_rec(n - 1) + fibonacci_rec(n - 2) # Recursive call \n",
    "```\n",
    "\n",
    "As you can see, the *n-th* Fibonacci number requires solving the *(n-1)* and *(n-2)* Fibonacci numbers, which in turn require solving the *(n-2)* and *(n-3)* Fibonacci numbers and the *(n-2)* and *(n-4)* Fibonacci numbers, and so on. \n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRS1aVQz5TVayZ1Sxtf1YZRguoml6Vplr70RhocJk1gCSzFwNB8hIaCk8_gJVHlP8DdTtS6xlvBYNAq/pub?w=4666&h=2732\" style=\"width:40%\">\n",
    "    <figcaption style=\"text-align:center\"><strong>Recursion tree for the n-th Fibonacci number:</strong></figcaption>   \n",
    "</figure>\n",
    "\n",
    "Looking at the recursion tree for Fibonacci numbers, it is clear that the majority of function calls make two other function calls. The number of function calls grows approximately by $2^n$, and so the upper bound on the complexity of `fibonacci_rec()` is $O\\left(2^n\\right)$, which is very very expensive. \n",
    "\n",
    "In general, any algorithm with complexity $O\\left(c^n\\right)$ for some constant $c$ has **exponential complexity**.\n",
    "\n",
    "## 2.4. Log Complexity $O(log \\ n)$\n",
    "\n",
    "Consider a function that takes a number $n$ and keeps dividing it by $2$ until the number becomes less than 1.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>TRY IT!</b> Write a function that counts the number of steps to perform the above operation for any input $n$. Determine the complexity of this function in Big-O notation.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4538e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function\n",
    "def divide_by_two(n):  \n",
    "    steps = 0 # start with 0 steps\n",
    "    while n >= 1: # repeat operation while n > 1\n",
    "        n = n / 2 # divide n by 2\n",
    "        steps += 1 # increase number of steps by 1\n",
    "    return steps\n",
    "\n",
    "# call the function\n",
    "divide_by_two(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb8258",
   "metadata": {},
   "source": [
    "Again, only the while-loop runs longer for larger $n$ so we can focus on the loop. Within the while-loop, there is an operation that repeats once for every iteration. So the complexity depends only on how many times the while loop runs. The while loop divides $n$ in half in every iteration until $n$ is less than 1. So the number of iterations, $i$, is the solution to the following equation: \n",
    "\n",
    "$$\\dfrac{n}{2\\times2\\times2 ...} = \\dfrac{n}{2^i} = 1 \\rightarrow 2^i = n$$ \n",
    "\n",
    "With some manipulation, this solves to $i = \\log_2 n$ (this is based on log-base-2, although it does not matter what the base of the log is). So the complexity of `divide_by_two()` is $O(log \\ n)$. Any algorithm with complexity $O(\\log n)$ has **log complexity**.\n",
    "\n",
    "It's worth noting that logarithmic time complexity is very efficient, and the number of steps doesn't grow significantly with large values of $n$.\n",
    "\n",
    "Let's consider another example. Recall that the linear search algorithm, which starts at one end and goes through each element of a list until the desired element is found, has a time complexity of $O(n)$, which is linear complexity. If the list was sorted, is there a better way to search for `x` in `L`?\n",
    "\n",
    "If we are searching for `x = 5` and we have a sorted list `L = [3, 4, 12, 15, 16, 21, 24, 50, 55, 79]`, there is no point searching past 12, since we know the list is sorted, and we are already past what we are looking for. A better search algorithm, known as binary search, takes advantage of the fact that the list is sorted by ignoring half of the items after just one comparison. Here is how it works:\n",
    "\n",
    "* Compare `x` with the middle item in `L`\n",
    "* If `x` matches with the middle item, return True\n",
    "* Else if `x` is greater than the middle item, then `x` can only be in the second (greater) half of the list\n",
    "* Then we apply the algorithm again for the right half\n",
    "* Else if `x` is smaller, then `x` can only be in the first (lower) half of the list\n",
    "* The process repeats until `x` is found or until the list can no longer be split in half\n",
    "\n",
    "See the code below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40132822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(x, L):\n",
    "    \n",
    "    low = 0 # index of first element\n",
    "    high = len(L)-1 # index of last element\n",
    "    \n",
    "    while low <= high: # repeat while list has mor ethan one item left\n",
    "        \n",
    "        mid = (low + high) // 2 # get quotient using floor division\n",
    "        \n",
    "        if L[mid] == x: # if item in middle index is equal to x return True\n",
    "            return True\n",
    "        else: # else keep searching, but choose either upper lower or greater half\n",
    "            if x < L[mid]:\n",
    "                high = mid - 1 # search between low and mid - 1 (lower half)\n",
    "            else:\n",
    "                low = mid + 1 # search between mid + 1 and high (greater half)\n",
    "    \n",
    "    return False # only gets here if x is not found in L after checking all halves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0632f6",
   "metadata": {},
   "source": [
    "Since binary search splits the list in half each time, there are far fewer comparisons or steps. Given a list of size $n=16$, it is not hard to see there will at worst be 5 comparisons (just think about splitting 16 in half five times: 16, 8, 4, 2, 1). Mathematically, this can be computed with log-base-2 of $n$. If the size of the list is now doubled, there is only one additional step required!\n",
    "\n",
    "$$log_2(16)=5$$ $$log_2(32)=6$$ $$log_2(64)=7$$ $$...$$\n",
    "\n",
    "So, for one-million items in a sorted Python list, binary search will require at most, about 20 steps. If we double the size of the list to two-million, binary search now requires only about 21 steps. This demonstrates the remarkable efficiency of binary search. This is very different from linear search. A linear search algorithm would require, at worst, two-million steps, compared to that of binary search, which is 21 steps. Thus, binary search is a much more efficient algorithm. The figure below compares the time complexity of linear search and binary search.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTlFHbQFLFREhS1fmvUZdnqx7WWJNg0a93dX7ItVCBbGA5-q_Ejm-zicyehAYME6EhgFwRrC9qQfLd5/pub?w=1249&h=394\" style=\"width:100%\">\n",
    "    <figcaption style=\"text-align:center\"><strong>Complexity of linear versus binary search algorithms:</strong> <a href=\"https://www.codecademy.com/learn/linear-data-structures-java/modules/searching-arrays-java/cheatsheet\">https://www.codecademy.com/</a></figcaption>   \n",
    "</figure>\n",
    "\n",
    "# 3. Comparison of Complexities \n",
    "\n",
    "The Big-O notation serves as a concise way to express how the number of operations in an algorithm scales as a function of the input size $n$. Figuring out the time complexity an algorithm takes is a lot easier than figuring out an exact formula. Throw away the smaller terms, and the multiplicative constants, and focus on the part that is changing as the input size $n$ changes.\n",
    "\n",
    "Here are some common complexities and what they signify:\n",
    "\n",
    "* $O(log\\ n)$: Only one extra step is added each time the size of the data doubles\n",
    "* $O(n)$: The number of steps doubles each time the size of the data doubles\n",
    "* $O(n^2)$: The number of steps quadruples each time the size of the data doubles\n",
    "\n",
    "So why does complexity matter? Because algorithms with different complexities require different times to complete a task. It's not only important that an algorithm works correctly; it's equally crucial that it completes the job efficiently. The following figure demonstrates how the run time changes with different input sizes for various complexities.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*5ZLci3SuR0zM_QlZOADv8Q.jpeg\" style=\"width:80%\">\n",
    "  <figcaption style=\"text-align:center\"><strong>Big-O complexity graph:</strong> <a href=\"http://bigocheatsheet.com/\">http://bigocheatsheet.com/</a></figcaption>   \n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
